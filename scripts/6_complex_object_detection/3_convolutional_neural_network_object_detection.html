

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Convolutional Neural Network Object Detection &#8212; Machine Vision</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'scripts/6_complex_object_detection/3_convolutional_neural_network_object_detection';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Convolutional Neural Network Image Segmentation" href="4_convolutional_neural_network_image_segmentation.html" />
    <link rel="prev" title="Convolutional Neural Network Image Classification: MNIST" href="2_convolutional_neural_network_image_classification_MNIST.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.jpg" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/logo.jpg" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Machine Vision Course
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Image processing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../1_image_processing/1_color_spaces.html">Color spaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_image_processing/2_image_analysis.html">Image Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_image_processing/3_monadic_operations.html">Monadic operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_image_processing/4_diadic_operations.html">Diadic operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_image_processing/5_spatial_operations.html">Spatial operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_image_processing/6_shape_changing.html">Shape changing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_image_processing/7_histogram_equalization.html">Histogram equalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_image_processing/8_thresholding.html">Thresholding</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_image_processing/9_morphological_transformations.html">Morphological transformations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Image formation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../2_image_formation/1_intrinsic_camera_calibration.html">Intrinsic camera calibration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_image_formation/2_extrinsic_camera_calibration.html">Extrinsic camera calibration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_image_formation/3_augmented_reality_application.html">Augmented reality application</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Feature extraction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../3_feature_extraction/1_feature_detection.html">Feature detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_feature_extraction/2_feature_matching.html">Feature matching</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_feature_extraction/3_panorama_application.html">Panorama application</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Stereo vision</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../4_stereo_vision/stereo_vision.html">Stereo vision</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Simple object detection</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../5_simple_object_detection/10_car_detection_application.html">Motion detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_simple_object_detection/11_fish_detection_application.html">Fish detection application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_simple_object_detection/1_edge_detection.html">Edge detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_simple_object_detection/2_contour_detection.html">Basic object detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_simple_object_detection/3_hough_transforms.html">Hough transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_simple_object_detection/4_simple_classifier.html">Simple classifier</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_simple_object_detection/5_color_detection.html">Color detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_simple_object_detection/6_haarcascades.html">Haarcascades</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_simple_object_detection/7_measuring_application.html">Measuring application</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_simple_object_detection/8_sawblade_application.html">Sawblade analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_simple_object_detection/9_crop_row_application.html">Crop row application</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Complex object detection</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1_feedforward_neural_network_image_classification_MNIST.html">Feedforward Neural Network Image Classification: MNIST</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_convolutional_neural_network_image_classification_MNIST.html">Convolutional Neural Network Image Classification: MNIST</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Convolutional Neural Network Object Detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_convolutional_neural_network_image_segmentation.html">Convolutional Neural Network Image Segmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="5_image_datasets.html">Image datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="6_dataset_creation.html">Dataset Creation</a></li>
<li class="toctree-l1"><a class="reference internal" href="7_transfer_learning.html">Transfer Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch_body_pose_estimator.html">Body pose estimator</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch_fashionMNIST.html">Pytorch on the FashionMNIST dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch_make_dataset.html">Make dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch_mnist.html">MNIST classicifation</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch_tensorboard.html">Tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch_transfer_learning.html">Transfer learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch_yolo_object_detection.html">Yolo object detection</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/scripts/6_complex_object_detection/3_convolutional_neural_network_object_detection.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Convolutional Neural Network Object Detection</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#faster-r-cnn">Faster R-CNN</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#import-libraries"><strong>Import Libraries</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><strong>Faster R-CNN</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-the-object-classes"><strong>Predicting the Object Classes</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#drawing-the-bounding-box"><strong>Drawing the Bounding Box</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#results"><strong>Results</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#yolo"><strong>YOLO</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><strong>Import Libraries</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#yolo-v5"><strong>YOLO v5</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#images"><strong>Images</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3"><strong>Results</strong></a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="convolutional-neural-network-object-detection">
<h1>Convolutional Neural Network Object Detection<a class="headerlink" href="#convolutional-neural-network-object-detection" title="Permalink to this heading">#</a></h1>
<p>In this notebook, state-of-the-art models for object detection are used on some example images. First, Faster R-CNN will be covered. Subsequently, YOLO will be used for object detection.</p>
<section id="faster-r-cnn">
<h2>Faster R-CNN<a class="headerlink" href="#faster-r-cnn" title="Permalink to this heading">#</a></h2>
<section id="import-libraries">
<h3><strong>Import Libraries</strong><a class="headerlink" href="#import-libraries" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="nn">transforms</span>
<span class="kn">import</span> <span class="nn">argparse</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">coco_names</span> <span class="kn">import</span> <span class="n">COCO_INSTANCE_CATEGORY_NAMES</span> <span class="k">as</span> <span class="n">coco_names</span> <span class="c1"># the coco_names python script contains the classes of the objects</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c:\Users\Quinten Danneels\Documents\WP2 notebooks\Test\venv\lib\site-packages\tqdm\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
</div>
</div>
</section>
<section id="id1">
<h3><strong>Faster R-CNN</strong><a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>The pre-trained Faster R-CNN model will be loaded here. The model has a ResNet50 base network and will be loaded from the torchvision module. The min_size argument denotes the minimum dimensions of the bounding boxes that surround the objects. Making this value smaller will result in more small object to be detected.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">detection</span><span class="o">.</span><span class="n">fasterrcnn_resnet50_fpn</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">min_size</span><span class="o">=</span><span class="mi">800</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c:\Users\Quinten Danneels\Documents\WP2 notebooks\Test\venv\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter &#39;pretrained&#39; is deprecated since 0.13 and may be removed in the future, please use &#39;weights&#39; instead.
  warnings.warn(
c:\Users\Quinten Danneels\Documents\WP2 notebooks\Test\venv\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for &#39;weights&#39; are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
</pre></div>
</div>
</div>
</div>
</section>
<section id="predicting-the-object-classes">
<h3><strong>Predicting the Object Classes</strong><a class="headerlink" href="#predicting-the-object-classes" title="Permalink to this heading">#</a></h3>
<p>A function is written here to detect objects and predict its classes and bounding boxes with the pre-trained Faster R-CNN.</p>
<p>The torchvision model takes as input an image in the form of a tensor with dimensions [batch_size x channels x height x width]. Therefore the image needs to be transformed to a tensor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">detection_threshold</span><span class="p">):</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># adding a batch dimension because we only work with single images</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">image</span><span class="p">)</span> 

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Boxes: </span><span class="si">{</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;boxes&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Labels: </span><span class="si">{</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Scores: </span><span class="si">{</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;scores&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># get all the predicted class labels</span>
    <span class="n">pred_classes</span> <span class="o">=</span> <span class="p">[</span><span class="n">coco_names</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()]</span>

    <span class="c1"># get all the scores for the predicted objects</span>
    <span class="n">pred_scores</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;scores&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># get all the predicted bounding boxes</span>
    <span class="n">pred_bboxes</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;boxes&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="c1"># if the score is above the pre-defined threshold, then the bounding box is considered</span>
    <span class="n">boxes</span> <span class="o">=</span> <span class="n">pred_bboxes</span><span class="p">[</span><span class="n">pred_scores</span> <span class="o">&gt;=</span> <span class="n">detection_threshold</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">boxes</span><span class="p">,</span> <span class="n">pred_classes</span><span class="p">,</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="drawing-the-bounding-box">
<h3><strong>Drawing the Bounding Box</strong><a class="headerlink" href="#drawing-the-bounding-box" title="Permalink to this heading">#</a></h3>
<p>A function is written here to draw the bounding boxes around the detected objects in the image.</p>
<p>In the image there can be many objects of different classes. Therefore, bounding boxes of similar classes need to have the same colour. This helps in visualising of the detections.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">COLORS</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">coco_names</span><span class="p">),</span><span class="mi">3</span><span class="p">))</span> <span class="c1"># 3 dimensional vectors for each label is created, the values range between 0 and 255</span>
<span class="n">COLORS</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[123.48075186,  73.92799626, 223.94542466],
       [163.97518283, 107.54725324, 149.67667593],
       [193.44352098, 169.79850867, 233.09999435],
       [166.54612272, 202.62034713, 185.52641595],
       [ 42.28771528,  22.76402092,   7.64095573],
       [137.30603117, 121.12516876,  52.64644786],
       [103.10595361,  92.73616978, 116.91804925],
       [ 94.02892301,  99.86068373, 105.2025249 ],
       [178.73768096, 192.16518422, 236.61198427],
       [ 27.65813369,  27.96572352, 183.29918919],
       [241.4392651 ,  66.53069029, 249.94982306],
       [244.72227273, 141.18222004, 180.99349117],
       [132.87836467, 162.88608138,  16.42931841],
       [ 89.28164382, 247.08779233, 187.04201613],
       [144.1918526 , 130.3833348 , 155.20736061],
       [ 97.35464615, 247.89973886, 126.42464466],
       [179.78433625, 215.86268628, 195.99486036],
       [148.00764123, 222.23412707, 236.69823219],
       [ 75.64559373, 117.1298518 , 109.81933698],
       [245.39843303, 150.83087482, 211.54693008],
       [161.90985283, 114.25380777, 183.92549161],
       [ 43.96184656,   0.94957651,  51.97469128],
       [152.41819912, 157.05157652, 111.51008915],
       [184.56870789, 211.33060037, 187.83460804],
       [236.59540274, 131.25909738, 115.82905767],
       [ 85.64277932,   8.80145254,   2.95272801],
       [171.98151078, 199.06907354, 218.8698213 ],
       [ 73.00738947, 166.89880896,   1.71346549],
       [108.19527964,  82.71777687,  43.40310048],
       [193.79202926,  38.29430396,  34.72216415],
       [ 65.68936299, 223.74230883, 229.47886157],
       [ 72.29022741,  42.98847809,  68.10561252],
       [169.71779762, 121.06039904,   7.83015654],
       [220.29271382, 140.21940735,  38.75597632],
       [166.47553471, 140.98364372, 172.01341224],
       [ 27.57840146,  89.92222651, 122.54365229],
       [ 65.86599708,  57.52754672, 165.63272974],
       [ 17.71663878, 243.78988894,  72.06006417],
       [ 65.9444263 , 130.82593892, 209.860734  ],
       [ 96.57579302, 248.1533201 ,  72.93435565],
       [218.52842253, 221.07564667, 111.06521917],
       [ 67.21028312, 203.10442559, 192.3776813 ],
       [195.84881538, 250.3633192 ,  78.64384289],
       [ 22.22812524,  88.40582387,  26.52321786],
       [ 85.23881684, 220.84738443, 235.88787912],
       [181.87272028,  24.08635934, 242.57382323],
       [165.93360094, 122.04674635, 139.05925135],
       [236.39810514,  76.48967421, 188.65491184],
       [161.48714391, 171.87921582,  88.57079791],
       [169.7045143 , 143.75965961, 134.77259795],
       [ 23.94401348,  92.08331487, 212.75820772],
       [ 39.36786216, 250.99464545,  77.63410672],
       [186.32599962, 232.24921086,  79.19870324],
       [ 17.66596465, 135.06804102,  15.36049411],
       [224.94037734, 112.63096749, 242.69190756],
       [158.90659281, 195.47960156, 114.50136956],
       [197.86866032, 227.65226322, 183.65836305],
       [ 24.30649936, 172.27506842, 122.23952118],
       [240.65123755, 105.70805709, 106.97188398],
       [  8.19266976, 107.0323251 , 215.77179101],
       [181.6661328 ,   7.90747176,  61.68978934],
       [217.256827  ,  84.51476855, 208.64165745],
       [235.65211747, 158.64660637, 249.22779496],
       [124.26321672, 126.30704029,  51.49661492],
       [135.90693904,  42.09980591,  31.02599129],
       [125.34800549, 139.89542174,  49.79129348],
       [216.6182456 ,  77.21882847,  63.39677409],
       [115.71166707, 171.03151394, 151.07631868],
       [172.1642383 , 125.09136   , 107.44316032],
       [249.96176316, 112.06417781,  73.044277  ],
       [133.68099202,  10.71074484, 101.56678678],
       [210.37566801, 226.16696897,  66.11321449],
       [200.55983929, 228.84943025,   0.55856074],
       [192.90370543, 165.53983011, 246.62855795],
       [224.75852445, 242.00427999, 143.54553178],
       [ 17.16260115, 148.69502982, 177.25238651],
       [ 37.76231693, 116.05925616,  29.09278619],
       [110.84151856,  56.46069745, 151.0798594 ],
       [156.45374601, 110.18704225,  54.4383307 ],
       [224.8925108 , 197.20867509, 102.17751309],
       [111.42961609,  18.21340972, 184.35000861],
       [ 20.45180576, 127.9085311 , 232.05123379],
       [217.67200359,  83.87037184, 140.79857524],
       [  4.01246071,  40.37210006, 194.12597108],
       [244.14527822, 164.22272252, 103.33441833],
       [ 56.78969099, 158.75703832,  46.05608003],
       [ 16.96334453, 178.31648957, 127.77718363],
       [242.51458808,  51.6791762 , 179.50764146],
       [211.81708472, 147.97546538,  53.73359829],
       [ 44.05248273, 151.72175175, 253.49340193],
       [164.7304751 , 197.00297028, 189.32407873]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">draw_boxes</span><span class="p">(</span><span class="n">boxes</span><span class="p">,</span><span class="n">classes</span><span class="p">,</span><span class="n">labels</span><span class="p">,</span><span class="n">image</span><span class="p">):</span>
    <span class="c1"># read the image with OpenCV</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">image</span><span class="p">),</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">box</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">boxes</span><span class="p">):</span>
        <span class="n">color</span> <span class="o">=</span> <span class="n">COLORS</span><span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
        <span class="n">start_x</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">box</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">start_y</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">box</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">end_x</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">box</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">end_y</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">box</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">image</span><span class="p">,(</span><span class="n">start_x</span><span class="p">,</span><span class="n">start_y</span><span class="p">),(</span><span class="n">end_x</span><span class="p">,</span><span class="n">end_y</span><span class="p">),</span><span class="n">color</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># rectangle needs the image, the starting box coordinates, the ending box coordinates, the color and the line thickness as input</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">putText</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="n">classes</span><span class="p">[</span><span class="n">i</span><span class="p">],(</span><span class="n">start_x</span><span class="p">,</span><span class="n">start_y</span><span class="o">-</span><span class="mi">5</span><span class="p">),</span> <span class="n">cv2</span><span class="o">.</span><span class="n">FONT_HERSHEY_SIMPLEX</span><span class="p">,</span><span class="mf">0.8</span><span class="p">,</span><span class="n">color</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># putText needs the image, the starting coordinates of the text, the font, the size, the text color and the letter thickness as input</span>

    <span class="k">return</span> <span class="n">image</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="results">
<h3><strong>Results</strong><a class="headerlink" href="#results" title="Permalink to this heading">#</a></h3>
<p>The Faster R-CNN model will be now used with the above defined functions to perform object detection on 3 image examples.</p>
<p>The Faster R-CNN model detects all the objects within the image. A bounding box is placed around the objects. These are annotated with only the class name.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">images_ex</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;horses.jpg&#39;</span><span class="p">,</span><span class="s1">&#39;people.jpg&#39;</span><span class="p">,</span><span class="s1">&#39;street.jpg&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>FasterRCNN(
  (transform): GeneralizedRCNNTransform(
      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
      Resize(min_size=(800,), max_size=1333, mode=&#39;bilinear&#39;)
  )
  (backbone): BackboneWithFPN(
    (body): IntermediateLayerGetter(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): FrozenBatchNorm2d(64, eps=0.0)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(64, eps=0.0)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(64, eps=0.0)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(256, eps=0.0)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): FrozenBatchNorm2d(256, eps=0.0)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(64, eps=0.0)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(64, eps=0.0)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(256, eps=0.0)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(64, eps=0.0)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(64, eps=0.0)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(256, eps=0.0)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(128, eps=0.0)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(128, eps=0.0)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(512, eps=0.0)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d(512, eps=0.0)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(128, eps=0.0)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(128, eps=0.0)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(512, eps=0.0)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(128, eps=0.0)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(128, eps=0.0)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(512, eps=0.0)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(128, eps=0.0)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(128, eps=0.0)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(512, eps=0.0)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=0.0)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=0.0)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=0.0)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d(1024, eps=0.0)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=0.0)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=0.0)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=0.0)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=0.0)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=0.0)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=0.0)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=0.0)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=0.0)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=0.0)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=0.0)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=0.0)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=0.0)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(256, eps=0.0)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(256, eps=0.0)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(1024, eps=0.0)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(512, eps=0.0)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(512, eps=0.0)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(2048, eps=0.0)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): FrozenBatchNorm2d(2048, eps=0.0)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(512, eps=0.0)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(512, eps=0.0)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(2048, eps=0.0)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): FrozenBatchNorm2d(512, eps=0.0)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): FrozenBatchNorm2d(512, eps=0.0)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): FrozenBatchNorm2d(2048, eps=0.0)
          (relu): ReLU(inplace=True)
        )
      )
    )
    (fpn): FeaturePyramidNetwork(
      (inner_blocks): ModuleList(
        (0): Conv2dNormActivation(
          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (1): Conv2dNormActivation(
          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (2): Conv2dNormActivation(
          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
        )
        (3): Conv2dNormActivation(
          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (layer_blocks): ModuleList(
        (0): Conv2dNormActivation(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (1): Conv2dNormActivation(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (2): Conv2dNormActivation(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (3): Conv2dNormActivation(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (extra_blocks): LastLevelMaxPool()
    )
  )
  (rpn): RegionProposalNetwork(
    (anchor_generator): AnchorGenerator()
    (head): RPNHead(
      (conv): Sequential(
        (0): Conv2dNormActivation(
          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (1): ReLU(inplace=True)
        )
      )
      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))
      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (roi_heads): RoIHeads(
    (box_roi_pool): MultiScaleRoIAlign(featmap_names=[&#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;], output_size=(7, 7), sampling_ratio=2)
    (box_head): TwoMLPHead(
      (fc6): Linear(in_features=12544, out_features=1024, bias=True)
      (fc7): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (box_predictor): FastRCNNPredictor(
      (cls_score): Linear(in_features=1024, out_features=91, bias=True)
      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)
    )
  )
)
</pre></div>
</div>
</div>
</div>
<p>Image example 1:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">images_ex</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">boxes</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">draw_boxes</span><span class="p">(</span><span class="n">boxes</span><span class="p">,</span><span class="n">classes</span><span class="p">,</span><span class="n">labels</span><span class="p">,</span><span class="n">image</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Boxes: tensor([[281.8718,  79.2813, 420.7066, 225.9756],
        [330.9542,  80.9836, 593.0887, 228.4178],
        [109.6094,  95.4172, 332.9265, 226.0391],
        [111.1188, 108.3175, 299.6678, 226.6196],
        [359.3416,  98.2549, 600.4446, 232.9423],
        [142.3595,  82.4780, 474.7170, 232.5669],
        [217.1376,  91.9965, 338.9271, 226.1556],
        [111.3001, 108.9320, 300.2287, 223.7775],
        [346.2997,  14.0222, 632.1669, 213.3478],
        [351.5600, 123.3581, 592.3284, 229.2103]], grad_fn=&lt;StackBackward0&gt;)
Labels: tensor([19, 19, 19, 18, 18, 19, 19, 21,  1, 22])
Scores: tensor([0.9977, 0.9913, 0.9898, 0.2787, 0.1807, 0.1448, 0.1275, 0.0993, 0.0854,
        0.0730], grad_fn=&lt;IndexBackward0&gt;)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage at 0x1ee3a585120&gt;
</pre></div>
</div>
<img alt="../../_images/6a0c5efc253815c46109675899dc4a221e5fabacd5c590c08a891d67d89c4220.png" src="../../_images/6a0c5efc253815c46109675899dc4a221e5fabacd5c590c08a891d67d89c4220.png" />
</div>
</div>
<p>Image example 2:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">images_ex</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">boxes</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">draw_boxes</span><span class="p">(</span><span class="n">boxes</span><span class="p">,</span><span class="n">classes</span><span class="p">,</span><span class="n">labels</span><span class="p">,</span><span class="n">image</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Boxes: tensor([[ 838.6357,  332.8714,  931.3898,  610.8371],
        [ 235.5437,  160.7106,  430.6569,  690.1383],
        [1036.0819,  286.9707, 1156.6218,  604.0554],
        [ 718.0005,  406.3850,  778.0782,  582.8207],
        [ 490.6573,  387.9689,  557.2424,  599.1245],
        [ 762.8722,  405.6624,  821.9858,  584.3357],
        [ 608.5616,  386.3052,  668.2618,  598.7282],
        [ 455.2066,  407.0004,  505.5713,  588.4600],
        [ 572.2786,  409.2903,  619.7392,  594.0989],
        [ 654.2797,  413.5545,  704.2628,  587.6578],
        [ 376.1682,  346.8563,  469.2232,  622.5776],
        [ 184.0398,  191.8036,  311.2568,  613.2949],
        [ 869.4252,  368.6363,  920.3807,  477.0748],
        [ 463.3396,  441.3140,  502.3931,  502.6069],
        [ 685.6664,  444.3586,  713.8292,  519.5699],
        [1125.1453,  457.0604, 1159.7241,  500.2275],
        [ 179.3643,  366.9853,  238.8592,  446.9797],
        [1077.9030,  328.4338, 1087.2565,  336.9096],
        [ 606.8297,  462.7871,  646.4382,  520.2723],
        [ 591.4882,  394.0873,  646.1456,  601.4103],
        [ 582.7982,  471.8315,  608.9168,  504.8882],
        [ 190.5133,  229.5212,  294.1514,  435.1716],
        [ 272.1345,  233.2624,  389.4356,  316.5598],
        [1076.8818,  326.5680, 1084.8353,  334.7568],
        [1076.5957,  330.3362, 1086.1099,  339.2859],
        [ 267.9199,  234.2086,  303.3983,  313.4419],
        [ 182.2680,  290.9092,  248.0474,  606.8738],
        [ 444.4969,  442.9612,  500.1387,  531.9547],
        [ 265.5374,  223.8680,  399.3688,  409.8998],
        [ 663.5272,  441.8121,  707.2194,  519.1763],
        [ 368.8487,  406.9128,  408.7021,  604.7890],
        [ 364.9154,  230.8734,  388.6153,  318.1840],
        [1079.2688,  324.9970, 1088.9220,  334.3479],
        [ 371.6683,  237.1387,  402.4030,  429.4454],
        [ 452.5699,  493.1789,  472.5578,  541.1098],
        [ 846.0768,  495.3790,  870.2686,  575.4498],
        [ 341.3663,  234.4845,  389.6394,  323.2491],
        [1079.9165,  331.8026, 1088.7949,  339.8935],
        [1081.1755,  327.7093, 1089.1256,  337.2557],
        [1074.9076,  328.8901, 1082.3829,  337.1688],
        [ 201.4164,  110.4208,  242.1925,  151.4852],
        [ 378.0235,  421.7130,  396.0757,  443.9825],
        [ 267.0450,  242.6031,  295.9886,  323.6700],
        [ 685.0259,  527.2994,  727.5939,  570.6238],
        [ 330.6585,  202.2557,  450.7218,  643.6895],
        [ 847.7819,  500.6789,  869.3420,  575.8315],
        [ 606.3312,  432.8838,  650.4195,  514.3807],
        [1076.7961,  332.1517, 1087.5544,  343.0010],
        [ 293.2908,  297.6859,  335.3050,  354.2381],
        [ 917.5045,  277.3173, 1173.4236,  508.0836],
        [ 619.4684,  387.9424,  633.0031,  415.3564],
        [ 861.8055,  374.0642,  916.3222,  449.5957],
        [ 872.6832,  376.2596,  910.2354,  449.4831],
        [ 264.3297,  215.0753,  400.9728,  431.4933],
        [ 609.6163,  431.2700,  650.5692,  511.6338],
        [1076.1874,  320.9078, 1087.6924,  333.7243],
        [ 559.4634,  518.8362,  582.5876,  559.7557],
        [ 626.0091,  424.2775,  646.0499,  440.4637],
        [ 425.8087,  360.0792,  489.9438,  587.2103],
        [ 359.1673,  315.8119,  416.9346,  627.9749],
        [ 667.2069,  443.1429,  712.4965,  520.7167],
        [ 180.7574,  363.7178,  235.5377,  449.9066],
        [ 843.9357,  443.4947,  870.9682,  579.7020],
        [ 339.4966,  231.9283,  412.0000,  434.1110],
        [ 727.3713,  438.8018,  770.7550,  511.5929],
        [ 582.7911,  471.0620,  609.3300,  505.2877],
        [ 368.1226,  231.4482,  430.2378,  433.4972],
        [ 745.7853,  437.6011,  773.3105,  499.8786],
        [ 668.2468,  425.5607,  720.4258,  570.3208],
        [ 558.1824,  515.4930,  584.2905,  559.8504],
        [ 362.3917,  227.8665,  390.9729,  429.1337],
        [ 715.9057,  499.7785,  731.8454,  517.3592],
        [ 749.9988,  437.8743,  773.7007,  508.0155],
        [ 659.7587,  446.9388,  694.0140,  510.3260],
        [ 769.5087,  450.7253,  793.0660,  486.3597],
        [ 414.2022,  345.2997,  471.6040,  552.5637],
        [ 211.6082,  260.1252,  253.8367,  367.4155],
        [1073.9493,  324.3900, 1080.2734,  333.5712],
        [ 370.8800,  239.9180,  403.5157,  424.2956],
        [ 725.6954,  438.4138,  765.6232,  516.1743],
        [ 838.3405,  373.5945,  865.6230,  508.7992],
        [ 621.1636,  386.5378,  635.8318,  404.9206],
        [ 462.0701,  359.2635,  490.6133,  405.8841],
        [ 240.2804,  200.2216,  311.0898,  336.6347],
        [ 309.3058,  233.9445,  401.5692,  347.0967],
        [ 685.1104,  499.3392,  730.4431,  566.8127],
        [ 817.5460,  490.9745,  850.7949,  575.2332],
        [ 361.0360,  230.7719,  391.5721,  420.1719],
        [ 563.2505,  416.8862,  596.1802,  564.4354],
        [ 684.2018,  451.3036,  717.9206,  522.8470],
        [ 483.5205,  446.8525,  506.5295,  505.7527],
        [ 846.2860,  372.7581,  919.3865,  523.3672],
        [ 665.2142,  422.9936,  680.1906,  445.5361],
        [ 923.6543,  288.4537, 1174.4845,  502.5115],
        [ 723.1340,  449.3366,  744.0955,  505.2619],
        [ 722.7778,  441.9757,  744.5987,  498.4233],
        [1075.0762,  322.0228, 1083.8934,  330.9146],
        [ 617.6600,  394.3866,  630.8707,  423.8289],
        [ 376.9502,  413.5646,  396.6335,  445.4344],
        [ 189.6607,  230.7472,  295.7137,  427.7721]],
       grad_fn=&lt;StackBackward0&gt;)
Labels: tensor([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, 31, 31, 31, 31, 31, 77,
        31,  1, 31, 31, 27, 77, 77, 27,  1, 31, 27, 31,  1, 27, 77, 31, 31, 31,
        27, 77, 77, 77, 64, 77, 32, 33,  1, 33, 27, 77, 77,  8,  1, 27, 31, 31,
        31, 77, 33, 77,  1,  1, 27, 44,  1, 31, 27, 27, 27, 27,  1, 31, 31, 31,
        31, 27, 31,  1, 31, 77, 27, 31, 31,  1,  1,  1, 27, 31, 33, 27,  1, 27,
        31, 31, 77,  3, 31, 27, 77,  1, 44, 27])
Scores: tensor([0.9995, 0.9986, 0.9986, 0.9970, 0.9966, 0.9966, 0.9939, 0.9896, 0.9882,
        0.9750, 0.9654, 0.9540, 0.9378, 0.9237, 0.9116, 0.9080, 0.7812, 0.7810,
        0.7532, 0.7439, 0.7231, 0.7100, 0.6692, 0.6403, 0.6004, 0.5869, 0.5862,
        0.5023, 0.4942, 0.4621, 0.4291, 0.3991, 0.3954, 0.3662, 0.3451, 0.3438,
        0.3399, 0.3290, 0.3194, 0.2803, 0.2803, 0.2733, 0.2375, 0.2302, 0.2220,
        0.2211, 0.2156, 0.2136, 0.1913, 0.1812, 0.1713, 0.1599, 0.1552, 0.1546,
        0.1524, 0.1510, 0.1498, 0.1419, 0.1308, 0.1286, 0.1272, 0.1266, 0.1263,
        0.1248, 0.1246, 0.1232, 0.1180, 0.1177, 0.1127, 0.1101, 0.1044, 0.1024,
        0.1024, 0.0958, 0.0933, 0.0904, 0.0867, 0.0842, 0.0838, 0.0827, 0.0816,
        0.0812, 0.0804, 0.0801, 0.0792, 0.0774, 0.0770, 0.0726, 0.0707, 0.0699,
        0.0687, 0.0684, 0.0663, 0.0662, 0.0643, 0.0616, 0.0613, 0.0610, 0.0608,
        0.0592], grad_fn=&lt;IndexBackward0&gt;)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage at 0x1ee510087f0&gt;
</pre></div>
</div>
<img alt="../../_images/320c52a1b1cc911689a4c2a3ac291f32a47c1c74f64bc81e7f8149d8aa325b64.png" src="../../_images/320c52a1b1cc911689a4c2a3ac291f32a47c1c74f64bc81e7f8149d8aa325b64.png" />
</div>
</div>
<p>Image example 3:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">images_ex</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">boxes</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">draw_boxes</span><span class="p">(</span><span class="n">boxes</span><span class="p">,</span><span class="n">classes</span><span class="p">,</span><span class="n">labels</span><span class="p">,</span><span class="n">image</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Boxes: tensor([[5.8550e+02, 3.0679e+02, 7.6959e+02, 7.9109e+02],
        [6.1061e+02, 5.0469e+02, 7.7416e+02, 8.3717e+02],
        [3.5637e+02, 3.8366e+02, 4.2221e+02, 4.4126e+02],
        [3.7228e-01, 4.1606e+02, 2.8784e+01, 4.6788e+02],
        [5.5278e+02, 3.9554e+02, 5.7382e+02, 4.6087e+02],
        [2.5864e+01, 4.2121e+02, 6.5340e+01, 4.4425e+02],
        [8.1940e+02, 3.7355e+02, 8.4451e+02, 4.4950e+02],
        [1.5357e+02, 3.3876e+02, 4.9642e+02, 6.5111e+02],
        [1.3568e+02, 4.1083e+02, 1.4943e+02, 4.4751e+02],
        [9.4219e+02, 3.0277e+02, 9.6331e+02, 3.2791e+02],
        [6.9452e+01, 4.1938e+02, 1.1694e+02, 4.4502e+02],
        [2.4149e+01, 4.2302e+02, 4.6692e+01, 4.4643e+02],
        [1.1634e+03, 3.8943e+02, 1.2014e+03, 4.3058e+02],
        [1.1246e+03, 3.8154e+02, 1.1546e+03, 4.3327e+02],
        [1.0262e+03, 3.8537e+02, 1.0831e+03, 4.3714e+02],
        [7.7204e+02, 3.7843e+02, 7.9312e+02, 4.2867e+02],
        [1.0726e+03, 3.8711e+02, 1.1134e+03, 4.3559e+02],
        [5.1745e+02, 3.8899e+02, 5.3731e+02, 4.5047e+02],
        [1.0016e+03, 4.0035e+02, 1.0397e+03, 4.3948e+02],
        [1.0550e+03, 3.8751e+02, 1.0829e+03, 4.3693e+02],
        [8.3366e+02, 3.7433e+02, 8.4858e+02, 4.3920e+02],
        [1.0585e+03, 3.8308e+02, 1.1016e+03, 4.3593e+02],
        [1.2117e+03, 3.9036e+02, 1.2298e+03, 4.2504e+02],
        [1.0983e+03, 3.8142e+02, 1.1677e+03, 4.3254e+02],
        [1.5557e+02, 3.9530e+02, 1.9362e+02, 4.6767e+02],
        [3.0550e+02, 3.7553e+02, 4.3128e+02, 4.4952e+02],
        [1.1512e+03, 3.9390e+02, 1.1716e+03, 4.2983e+02],
        [1.5685e+02, 4.1208e+02, 1.8200e+02, 4.6634e+02],
        [9.9666e+02, 3.9287e+02, 1.0618e+03, 4.3893e+02],
        [1.1080e+03, 3.8814e+02, 1.1485e+03, 4.3400e+02],
        [1.2269e+03, 3.9033e+02, 1.2438e+03, 4.2081e+02],
        [1.1984e+03, 3.9182e+02, 1.2154e+03, 4.2594e+02],
        [1.5517e+02, 3.8732e+02, 2.0264e+02, 4.6407e+02],
        [1.1287e+03, 3.7603e+02, 1.1722e+03, 4.3258e+02],
        [9.8259e+02, 3.8437e+02, 1.1197e+03, 4.3880e+02],
        [1.0864e+03, 3.9051e+02, 1.1217e+03, 4.3016e+02],
        [1.2263e+03, 3.8682e+02, 1.2437e+03, 4.0569e+02],
        [2.1463e+02, 3.9655e+02, 2.9263e+02, 4.5107e+02],
        [8.8263e+02, 3.9737e+02, 9.3235e+02, 4.4997e+02],
        [8.1336e+01, 4.1708e+02, 1.0676e+02, 4.4261e+02],
        [1.0852e+03, 3.8168e+02, 1.2533e+03, 4.3311e+02],
        [9.8770e+02, 3.9219e+02, 1.0300e+03, 4.3999e+02],
        [7.4154e+02, 3.8357e+02, 7.6387e+02, 4.4071e+02],
        [1.1523e+03, 3.9129e+02, 1.2304e+03, 4.2987e+02],
        [8.3603e+02, 3.7529e+02, 8.4795e+02, 4.1554e+02],
        [1.2120e+03, 3.8944e+02, 1.2272e+03, 4.1127e+02],
        [1.1721e+03, 3.7817e+02, 1.1963e+03, 4.1008e+02],
        [1.1962e+03, 3.9084e+02, 1.2262e+03, 4.2645e+02],
        [1.1962e+03, 3.8977e+02, 1.2110e+03, 4.1698e+02],
        [5.2585e+02, 4.0056e+02, 5.3928e+02, 4.4555e+02],
        [1.0573e+03, 3.9347e+02, 1.1336e+03, 4.3642e+02],
        [1.1819e+03, 3.8914e+02, 1.2633e+03, 4.2760e+02],
        [1.5587e+02, 3.8797e+02, 2.0235e+02, 4.6774e+02],
        [9.4006e+02, 3.5870e+02, 9.5586e+02, 3.7521e+02],
        [4.6566e+02, 4.0135e+02, 4.9811e+02, 4.5680e+02],
        [1.0933e+03, 3.7991e+02, 1.1339e+03, 4.3445e+02],
        [2.9826e+02, 3.7356e+02, 3.7803e+02, 4.4405e+02],
        [1.2403e+03, 4.0188e+02, 1.2621e+03, 4.2015e+02],
        [1.5349e+02, 3.3968e+02, 4.8313e+02, 6.5744e+02],
        [1.7005e+02, 4.0690e+02, 1.8752e+02, 4.2956e+02],
        [9.7334e+01, 4.2286e+02, 1.1867e+02, 4.4560e+02],
        [1.2403e+03, 3.8760e+02, 1.2563e+03, 4.0489e+02],
        [1.2281e+03, 4.0230e+02, 1.2464e+03, 4.2493e+02],
        [1.6307e+02, 4.0976e+02, 1.8048e+02, 4.2873e+02],
        [1.2126e+03, 3.9343e+02, 1.2309e+03, 4.2639e+02],
        [1.2370e+03, 3.9011e+02, 1.2630e+03, 4.2078e+02],
        [1.2296e+03, 4.0095e+02, 1.2454e+03, 4.2252e+02],
        [1.1541e+03, 3.8903e+02, 1.1754e+03, 4.1719e+02],
        [8.2870e+02, 3.7584e+02, 8.4469e+02, 4.2732e+02],
        [1.0729e+03, 3.6333e+02, 1.1188e+03, 4.1811e+02],
        [6.6352e+02, 3.8757e+02, 6.7415e+02, 4.0271e+02],
        [1.2222e+03, 3.9061e+02, 1.2368e+03, 4.2333e+02],
        [1.1096e+02, 4.2332e+02, 1.2249e+02, 4.4217e+02],
        [7.6405e+02, 3.7582e+02, 7.9457e+02, 4.5095e+02],
        [1.2402e+03, 4.0247e+02, 1.2638e+03, 4.2145e+02],
        [1.1313e+03, 3.7563e+02, 1.2103e+03, 4.2765e+02],
        [1.2264e+03, 3.8857e+02, 1.2712e+03, 4.2548e+02],
        [2.4748e+02, 3.9344e+02, 3.0967e+02, 4.4878e+02],
        [1.2637e+03, 4.0190e+02, 1.2787e+03, 4.2431e+02],
        [8.9094e+02, 3.9225e+02, 9.9276e+02, 4.4794e+02],
        [9.3267e+02, 3.5530e+02, 9.6221e+02, 4.5036e+02],
        [1.2313e+03, 3.8949e+02, 1.2504e+03, 4.1434e+02],
        [1.1957e+03, 3.9006e+02, 1.2124e+03, 4.1681e+02]],
       grad_fn=&lt;StackBackward0&gt;)
Labels: tensor([ 1,  2,  1,  3,  1,  3,  1,  3,  1, 13,  3,  3,  4,  4,  4,  1,  4,  1,
         4,  4,  1,  4,  1,  4,  3,  1,  4,  1,  4,  4,  1,  1,  6,  4,  4,  4,
         1,  1,  2,  3,  4,  4,  1,  4,  1,  1,  1,  4,  1,  1,  4,  4,  8,  1,
         3,  4,  1, 20,  8,  1,  3,  1, 18,  1,  4,  1, 20,  4,  1,  4, 32,  1,
         3,  1, 18,  4,  4,  1,  3,  2,  1,  1,  4])
Scores: tensor([0.9993, 0.9897, 0.9853, 0.9846, 0.9822, 0.9693, 0.9591, 0.9528, 0.9502,
        0.9481, 0.8607, 0.8132, 0.7992, 0.7860, 0.7467, 0.7184, 0.6709, 0.6695,
        0.6543, 0.6056, 0.5890, 0.5620, 0.5156, 0.5118, 0.5082, 0.5060, 0.4516,
        0.4117, 0.3965, 0.3844, 0.3780, 0.3733, 0.3621, 0.3173, 0.3076, 0.3065,
        0.3042, 0.3015, 0.2972, 0.2746, 0.2706, 0.2693, 0.2630, 0.2372, 0.2309,
        0.2237, 0.2197, 0.2137, 0.2052, 0.1962, 0.1754, 0.1738, 0.1718, 0.1610,
        0.1467, 0.1434, 0.1353, 0.1346, 0.1321, 0.1277, 0.1177, 0.1122, 0.1065,
        0.1005, 0.0976, 0.0969, 0.0958, 0.0910, 0.0897, 0.0870, 0.0862, 0.0861,
        0.0836, 0.0832, 0.0772, 0.0721, 0.0614, 0.0586, 0.0563, 0.0533, 0.0531,
        0.0516, 0.0511], grad_fn=&lt;IndexBackward0&gt;)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage at 0x1ee51072d10&gt;
</pre></div>
</div>
<img alt="../../_images/145d1ecf817ca11749cf90b0ef93f0504b3f4c1f54f9f98bf84b902ffa1ded03.png" src="../../_images/145d1ecf817ca11749cf90b0ef93f0504b3f4c1f54f9f98bf84b902ffa1ded03.png" />
</div>
</div>
</section>
</section>
<section id="yolo">
<h2><strong>YOLO</strong><a class="headerlink" href="#yolo" title="Permalink to this heading">#</a></h2>
<section id="id2">
<h3><strong>Import Libraries</strong><a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="yolo-v5">
<h3><strong>YOLO v5</strong><a class="headerlink" href="#yolo-v5" title="Permalink to this heading">#</a></h3>
<p>The pre-trained YOLO v5 model will be loaded here from ultralytics. But first install the model requirements in your virtual environment: pip install -r <a class="reference external" href="https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt">https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;ultralytics/yolov5&#39;</span><span class="p">,</span> <span class="s1">&#39;yolov5s&#39;</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Using cache found in C:\Users\Quinten Danneels/.cache\torch\hub\ultralytics_yolov5_master
YOLOv5  2023-2-8 Python-3.10.10 torch-1.13.1+cpu CPU

Fusing layers... 
YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients
Adding AutoShape... 
</pre></div>
</div>
</div>
</div>
</section>
<section id="images">
<h3><strong>Images</strong><a class="headerlink" href="#images" title="Permalink to this heading">#</a></h3>
<p>Some images from ultralytics are loaded here to perform object detection on.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">examples</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;zidane.jpg&#39;</span><span class="p">,</span> <span class="s1">&#39;bus.jpg&#39;</span><span class="p">,</span> <span class="p">]</span>
<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">download_url_to_file</span><span class="p">(</span><span class="s1">&#39;https://ultralytics.com/images/&#39;</span> <span class="o">+</span> <span class="n">f</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span> 
<span class="n">im1</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;zidane.jpg&#39;</span><span class="p">)</span>  <span class="c1"># PIL image</span>
<span class="n">im2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;bus.jpg&#39;</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># OpenCV image (BGR to RGB)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 165k/165k [00:00&lt;00:00, 3.20MB/s]
100%|██████████| 476k/476k [00:00&lt;00:00, 5.23MB/s]
</pre></div>
</div>
</div>
</div>
</section>
<section id="id3">
<h3><strong>Results</strong><a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<p>The YOLO v5 model is now used to perform object detection on the 2 downloaded image examples. Batch interference is used.</p>
<p>The YOLO v5 model detects all the objects within the image. A bounding box is placed around the objects. These are annotated with the class name and probability.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">([</span><span class="n">im1</span><span class="p">,</span> <span class="n">im2</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">640</span><span class="p">)</span> <span class="c1"># size = batch of images</span>
<span class="n">results</span><span class="o">.</span><span class="n">print</span><span class="p">()</span>  
<span class="n">results</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>image 1/2: 720x1280 2 persons, 2 ties
image 2/2: 1080x810 4 persons, 1 bus
Speed: 11.9ms pre-process, 108.4ms inference, 1.0ms NMS per image at shape (2, 3, 640, 640)
</pre></div>
</div>
<img alt="../../_images/67d46d61101ca7b2c9fc4e4ee030adfdb3b79e116b67e52c9a2f3fc051a7074a.png" src="../../_images/67d46d61101ca7b2c9fc4e4ee030adfdb3b79e116b67e52c9a2f3fc051a7074a.png" />
<img alt="../../_images/dc460e5a612bb38f71d8db9812aa5178e47257e8ab487c265e65ece7d21faa67.png" src="../../_images/dc460e5a612bb38f71d8db9812aa5178e47257e8ab487c265e65ece7d21faa67.png" />
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./scripts\6_complex_object_detection"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="2_convolutional_neural_network_image_classification_MNIST.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Convolutional Neural Network Image Classification: MNIST</p>
      </div>
    </a>
    <a class="right-next"
       href="4_convolutional_neural_network_image_segmentation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Convolutional Neural Network Image Segmentation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#faster-r-cnn">Faster R-CNN</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#import-libraries"><strong>Import Libraries</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><strong>Faster R-CNN</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-the-object-classes"><strong>Predicting the Object Classes</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#drawing-the-bounding-box"><strong>Drawing the Bounding Box</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#results"><strong>Results</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#yolo"><strong>YOLO</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><strong>Import Libraries</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#yolo-v5"><strong>YOLO v5</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#images"><strong>Images</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3"><strong>Results</strong></a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Matthias De Ryck
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>