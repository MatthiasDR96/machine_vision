{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification using convolutional layers with Keras\n",
    "In this notebook, a Convolutional Neural Network will be build from scratch and trained on the MNIST dataset. The MNIST dataset is a dataset containing hand-written images of digits. The end goal is to label the images with their corresponding digit. Take a look at the Feedforward Neural Network Image Classification of the MNIST dataset for the exploring of the MNIST dataset and the explanation about the data preparation.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "First, the MNIST data is loaded from the Keras dataset library. It directly loads in a training and test dataset. X denotes the images and y denotes the labels. We can also have a look at the size of these datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset contains 60000 samples.\n",
      "The test dataset contains 10000 samples.\n"
     ]
    }
   ],
   "source": [
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Print results\n",
    "print('The train dataset contains ' + str(x_train.shape[0]) + ' samples.')\n",
    "print('The test dataset contains ' + str(x_test.shape[0]) + ' samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the images are converted to doubles and normalized. Initially, the images have pixel values ranging from 0 to 255 (8 bits). But for the network to train faster, it is always beneficial to normalize the input values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels of the dataset are numbers from 0 to 9. It is common practice in neural network training to convert these numbers to binary vectors  with a 1 at the index of the correct number and zeros everywhere else. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label of the first image is: 5\n",
      "The label of the first image is: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# The labels of the dataset are the correct numbers for each image\n",
    "print(\"The label of the first image is: \" + str(y_train[0]))\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# The labels are converted to binary vectors\n",
    "print(\"The new label of the first image is: \" + str(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the neural network model\n",
    "As a neural network, we use a sequence of 2 convolutional layers followed by a pooling layer. The first convolutional layer contains 32 filters with size 3x3 and has a ReLu function as activation. The first convolutional layer contains 64 filters with size 3x3 and also has a ReLu function as activation. These two convolutional layers together are the feature extraction part of the neural network. A dropout layer is included to prevent overfitting. After these layers there is a fully connected (dense) layer that represents the classification part of the neural network. This final layer outputs 10 neurons that each represent a class instance, being a number from 0 to 9. The activation is a SoftMax layer that outputs a probability for each of the classes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
        "output_scroll"
    ]
    },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                16010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34826 (136.04 KB)\n",
      "Trainable params: 34826 (136.04 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Defines the neural network model\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Summarizes the model and its parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "Before we can train the model, we need to define what loss, optimizer, and accuracy metric we want to use. As we are dealing with a multi-class classification problem, we use the categorical cross entropy loss. As optimizer we use the popular Adam algorithm. And we will define accuracy as the metric of goodness for our network. We also define that we will validate the network after every epoch on a dataset that has 10% of the samples in the training dataset that was defined earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
        "output_scroll"
    ]
    },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 35s 79ms/step - loss: 0.3643 - accuracy: 0.8919 - val_loss: 0.0871 - val_accuracy: 0.9753\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 33s 79ms/step - loss: 0.1120 - accuracy: 0.9659 - val_loss: 0.0584 - val_accuracy: 0.9852\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 33s 79ms/step - loss: 0.0853 - accuracy: 0.9742 - val_loss: 0.0529 - val_accuracy: 0.9853\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 34s 82ms/step - loss: 0.0716 - accuracy: 0.9783 - val_loss: 0.0436 - val_accuracy: 0.9882\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 35s 83ms/step - loss: 0.0633 - accuracy: 0.9808 - val_loss: 0.0382 - val_accuracy: 0.9892\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 34s 81ms/step - loss: 0.0556 - accuracy: 0.9826 - val_loss: 0.0334 - val_accuracy: 0.9912\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 30s 72ms/step - loss: 0.0507 - accuracy: 0.9839 - val_loss: 0.0345 - val_accuracy: 0.9908\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 33s 77ms/step - loss: 0.0490 - accuracy: 0.9843 - val_loss: 0.0337 - val_accuracy: 0.9908\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 0.0438 - accuracy: 0.9861 - val_loss: 0.0314 - val_accuracy: 0.9907\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 32s 77ms/step - loss: 0.0425 - accuracy: 0.9864 - val_loss: 0.0326 - val_accuracy: 0.9913\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 31s 75ms/step - loss: 0.0396 - accuracy: 0.9874 - val_loss: 0.0286 - val_accuracy: 0.9918\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 33s 79ms/step - loss: 0.0370 - accuracy: 0.9877 - val_loss: 0.0292 - val_accuracy: 0.9918\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 34s 81ms/step - loss: 0.0344 - accuracy: 0.9889 - val_loss: 0.0302 - val_accuracy: 0.9920\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 33s 79ms/step - loss: 0.0337 - accuracy: 0.9889 - val_loss: 0.0280 - val_accuracy: 0.9927\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 0.0330 - accuracy: 0.9892 - val_loss: 0.0289 - val_accuracy: 0.9923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ef9181ddf0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training parameters\n",
    "epochs = 15\n",
    "batch_size = 128\n",
    "\n",
    "# Define the loss function, optimizer, and accuracy metric\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation\n",
    "When the model is trained, we can evalueate its performance on the test set. By calling the evaluate-function of the model it uses the earlier defined goodness metric for the evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.023343771696090698\n",
      "Test accuracy: 0.9911999702453613\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "# Print results\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
