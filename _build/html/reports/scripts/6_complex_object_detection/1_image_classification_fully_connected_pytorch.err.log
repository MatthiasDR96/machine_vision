Traceback (most recent call last):
  File "C:\Python310\lib\site-packages\jupyter_cache\executors\utils.py", line 58, in single_nb_execution
    executenb(
  File "C:\Users\matth\AppData\Roaming\Python\Python310\site-packages\nbclient\client.py", line 1314, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "C:\Users\matth\AppData\Roaming\Python\Python310\site-packages\jupyter_core\utils\__init__.py", line 173, in wrapped
    return loop.run_until_complete(inner)
  File "C:\Python310\lib\asyncio\base_events.py", line 649, in run_until_complete
    return future.result()
  File "C:\Users\matth\AppData\Roaming\Python\Python310\site-packages\nbclient\client.py", line 709, in async_execute
    await self.async_execute_cell(
  File "C:\Users\matth\AppData\Roaming\Python\Python310\site-packages\nbclient\client.py", line 1062, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "C:\Users\matth\AppData\Roaming\Python\Python310\site-packages\nbclient\client.py", line 918, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
# Set the optimizer
optimizer = SGD(model.parameters(), lr=0.001)

# Set the loss function
criterium = nn.CrossEntropyLoss()

# Train for 50 epochs
results = train_model(model, optimizer, criterium, n_epochs=10)
------------------

----- stdout -----
Epoch 1/10
----------
------------------

[1;31m---------------------------------------------------------------------------[0m
[1;31mRuntimeError[0m                              Traceback (most recent call last)
Cell [1;32mIn[8], line 8[0m
[0;32m      5[0m criterium [38;5;241m=[39m nn[38;5;241m.[39mCrossEntropyLoss()
[0;32m      7[0m [38;5;66;03m# Train for 50 epochs[39;00m
[1;32m----> 8[0m results [38;5;241m=[39m [43mtrain_model[49m[43m([49m[43mmodel[49m[43m,[49m[43m [49m[43moptimizer[49m[43m,[49m[43m [49m[43mcriterium[49m[43m,[49m[43m [49m[43mn_epochs[49m[38;5;241;43m=[39;49m[38;5;241;43m10[39;49m[43m)[49m

Cell [1;32mIn[7], line 39[0m, in [0;36mtrain_model[1;34m(model, optimizer, criterum, n_epochs)[0m
[0;32m     36[0m labels [38;5;241m=[39m labels[38;5;241m.[39mto(device)  
[0;32m     38[0m [38;5;66;03m# Forward pass through the network[39;00m
[1;32m---> 39[0m outputs [38;5;241m=[39m [43mmodel[49m[43m([49m[43minputs[49m[43m)[49m
[0;32m     41[0m [38;5;66;03m# Softmax transforms the output probabilities into one selected class[39;00m
[0;32m     42[0m _, class_pred [38;5;241m=[39m torch[38;5;241m.[39mmax(outputs, [38;5;241m1[39m)

File [1;32mC:\Python310\lib\site-packages\torch\nn\modules\module.py:1518[0m, in [0;36mModule._wrapped_call_impl[1;34m(self, *args, **kwargs)[0m
[0;32m   1516[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[0;32m   1517[0m [38;5;28;01melse[39;00m:
[1;32m-> 1518[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)

File [1;32mC:\Python310\lib\site-packages\torch\nn\modules\module.py:1527[0m, in [0;36mModule._call_impl[1;34m(self, *args, **kwargs)[0m
[0;32m   1522[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[0;32m   1523[0m [38;5;66;03m# this function, and just call forward.[39;00m
[0;32m   1524[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[0;32m   1525[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[0;32m   1526[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[1;32m-> 1527[0m     [38;5;28;01mreturn[39;00m forward_call([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)
[0;32m   1529[0m [38;5;28;01mtry[39;00m:
[0;32m   1530[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

Cell [1;32mIn[5], line 14[0m, in [0;36mNeuralNet.forward[1;34m(self, x)[0m
[0;32m     12[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;28mself[39m,x):
[0;32m     13[0m     x [38;5;241m=[39m x[38;5;241m.[39mview([38;5;241m-[39m[38;5;241m1[39m, [38;5;241m28[39m[38;5;241m*[39m[38;5;241m*[39m[38;5;241m2[39m) [38;5;66;03m# First it flattens the image of 28 by 28 to a vector of 28*28 elements because we have a fully connected layer at the beginning[39;00m
[1;32m---> 14[0m     x [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mR([38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mlinear1[49m[43m([49m[43mx[49m[43m)[49m) [38;5;66;03m# Passing the vector to the first layer, activating the results, and obtaining the outputs[39;00m
[0;32m     15[0m     x [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mR([38;5;28mself[39m[38;5;241m.[39mlinear2(x)) [38;5;66;03m# Passing the outputs of the previous layer to the second layer, activating the results, and obtaining the outputs[39;00m
[0;32m     16[0m     x [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mlinear3(x) [38;5;66;03m# Passing the outputs of the previous layer to the final layer and obtaining the outputs[39;00m

File [1;32mC:\Python310\lib\site-packages\torch\nn\modules\module.py:1518[0m, in [0;36mModule._wrapped_call_impl[1;34m(self, *args, **kwargs)[0m
[0;32m   1516[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[0;32m   1517[0m [38;5;28;01melse[39;00m:
[1;32m-> 1518[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)

File [1;32mC:\Python310\lib\site-packages\torch\nn\modules\module.py:1527[0m, in [0;36mModule._call_impl[1;34m(self, *args, **kwargs)[0m
[0;32m   1522[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[0;32m   1523[0m [38;5;66;03m# this function, and just call forward.[39;00m
[0;32m   1524[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[0;32m   1525[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[0;32m   1526[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[1;32m-> 1527[0m     [38;5;28;01mreturn[39;00m forward_call([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)
[0;32m   1529[0m [38;5;28;01mtry[39;00m:
[0;32m   1530[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [1;32mC:\Python310\lib\site-packages\torch\nn\modules\linear.py:114[0m, in [0;36mLinear.forward[1;34m(self, input)[0m
[0;32m    113[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;28mself[39m, [38;5;28minput[39m: Tensor) [38;5;241m-[39m[38;5;241m>[39m Tensor:
[1;32m--> 114[0m     [38;5;28;01mreturn[39;00m [43mF[49m[38;5;241;43m.[39;49m[43mlinear[49m[43m([49m[38;5;28;43minput[39;49m[43m,[49m[43m [49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mweight[49m[43m,[49m[43m [49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mbias[49m[43m)[49m

[1;31mRuntimeError[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)

